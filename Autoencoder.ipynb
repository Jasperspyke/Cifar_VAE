{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#adjustable hparams\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "max_epochs = 1\n",
    "latent_dim = 128\n",
    "#change this to 'cuda'\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "dataset_path = '/Users/gpnuser/Documents/Horos Data/Jas_Pycharm/VAE'\n",
    "loss_log = np.zeros(max_epochs,)\n",
    "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=False)\n",
    "np.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=True, nanstr=None, infstr=None, formatter=None, sign=None, floatmode=None, legacy=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stride = 2\n",
    "padding = 1\n",
    "#dont touch this stuff\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = CIFAR10(root=dataset_path, train=True, transform=transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=dataset_path, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "x = torch.load('in_imagetwo.pt')\n",
    "#plt.imshow(x[0].permute(1,2,0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# layer architecture\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=stride, padding=padding),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=stride, padding=padding),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=stride, padding=padding),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "# \t\t\tnn.Conv2d(48, 96, 4, stride=stride, padding=padding),           # [batch, 96, 2, 2]\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(96, 48, 4, stride=stride, padding=padding),  # [batch, 48, 4, 4]\n",
    "#             nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=stride, padding=padding),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=stride, padding=padding),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=stride, padding=padding),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "  #      plt.imshow(reconstructed[0].permute(1,2,0))\n",
    "        return latent, reconstructed\n",
    "\n",
    "    def get_loss(self, batch):\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(x, x_hat)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch)\n",
    "        #implement log loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch)\n",
    "        #implement log loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def represent(self):\n",
    "        print('encoder is:')\n",
    "        print(self.encoder)\n",
    "        print('decoder is:')\n",
    "        print(self.decoder)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/gpnuser/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/Users/gpnuser/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 23.7 K\n",
      "1 | decoder | Sequential | 23.7 K\n",
      "---------------------------------------\n",
      "47.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.4 K    Total params\n",
      "0.189     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder is:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      ")\n",
      "decoder is:\n",
      "Sequential(\n",
      "  (0): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dbbbe7dad6847e0acf340db8936bdb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m net\u001B[38;5;241m.\u001B[39mrepresent()\n\u001B[1;32m      4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer()\n\u001B[0;32m----> 5\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:608\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    606\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 608\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     41\u001B[0m     trainer\u001B[38;5;241m.\u001B[39m_call_teardown_hook()\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    643\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    644\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_set_ckpt_path(\n\u001B[1;32m    645\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    646\u001B[0m     ckpt_path,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    647\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    648\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    649\u001B[0m )\n\u001B[0;32m--> 650\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1103\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[1;32m   1101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1103\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1105\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1182\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1182\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1195\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_training_routine()\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1195\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[38;5;66;03m# enable train mode\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1267\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m-> 1267\u001B[0m     \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1269\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1271\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001B[0m, in \u001B[0;36mEvaluationLoop.advance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_dataloaders \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    151\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataloader_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m dataloader_idx\n\u001B[0;32m--> 152\u001B[0m dl_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_max_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;66;03m# store batch level output per dataloader\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs\u001B[38;5;241m.\u001B[39mappend(dl_outputs)\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001B[0m, in \u001B[0;36mEvaluationEpochLoop.advance\u001B[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# lightning module methods\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step_end(output)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001B[0m, in \u001B[0;36mEvaluationEpochLoop._evaluation_step\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \n\u001B[1;32m    225\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;124;03m    the outputs of the step\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    233\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 234\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1485\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1485\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1487\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m   1488\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mval_step_context():\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, ValidationStep)\n\u001B[0;32m--> 390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 46\u001B[0m, in \u001B[0;36mAutoencoder.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m---> 46\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 36\u001B[0m, in \u001B[0;36mAutoencoder.get_loss\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     34\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m batch  \u001B[38;5;66;03m# We do not need the labels\u001B[39;00m\n\u001B[1;32m     35\u001B[0m x_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(x)\n\u001B[0;32m---> 36\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_hat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(loss)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/torch/nn/functional.py:3281\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[1;32m   3278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3279\u001B[0m         mse_loss, (\u001B[38;5;28minput\u001B[39m, target), \u001B[38;5;28minput\u001B[39m, target, size_average\u001B[38;5;241m=\u001B[39msize_average, reduce\u001B[38;5;241m=\u001B[39mreduce, reduction\u001B[38;5;241m=\u001B[39mreduction\n\u001B[1;32m   3280\u001B[0m     )\n\u001B[0;32m-> 3281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[1;32m   3282\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   3283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3284\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()),\n\u001B[1;32m   3286\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   3287\u001B[0m     )\n\u001B[1;32m   3288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = Autoencoder()\n",
    "    net.represent()\n",
    "    trainer = pl.Trainer()\n",
    "    trainer.fit(net, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5031, 0.5030, 0.5037,  ..., 0.5028, 0.5038, 0.5021],\n",
      "        [0.5021, 0.5031, 0.5028,  ..., 0.5032, 0.5025, 0.5033],\n",
      "        [0.5033, 0.5008, 0.5050,  ..., 0.5008, 0.5048, 0.5003],\n",
      "        ...,\n",
      "        [0.5020, 0.5030, 0.5025,  ..., 0.5028, 0.5030, 0.5034],\n",
      "        [0.5033, 0.5009, 0.5050,  ..., 0.5009, 0.5050, 0.5006],\n",
      "        [0.5010, 0.5042, 0.5001,  ..., 0.5043, 0.5000, 0.5030]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "middle, end = net.forward(x)\n",
    "end = end[0].detach()\n",
    "plt.imshow(end.permute(1,2,0))\n",
    "print(end[0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
