{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#adjustable hparams\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "max_epochs = 120\n",
    "latent_dim = 128\n",
    "#change this to 'cuda'\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "dataset_path = '/Users/gpnuser/Documents/Horos Data/Jas_Pycharm/VAE'\n",
    "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=False)\n",
    "np.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=True, nanstr=None, infstr=None, formatter=None, sign=None, floatmode=None, legacy=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stride = 2\n",
    "padding = 1\n",
    "kernel_size = 3\n",
    "#dont touch this stuff\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "#transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "transform = transforms.ToTensor()\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = CIFAR10(root=dataset_path, train=True, transform=transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=dataset_path, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# layer architecture\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=stride, padding=padding),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=stride, padding=padding),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=stride, padding=padding),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "# \t\t\tnn.Conv2d(48, 96, 4, stride=stride, padding=padding),           # [batch, 96, 2, 2]\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(96,latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(96, 48, 4, stride=stride, padding=padding),  # [batch, 48, 4, 4]\n",
    "#             nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=stride, padding=padding),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=stride, padding=padding),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=stride, padding=padding),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "  #      plt.imshow(reconstructed[0].detach().numpy().permute(1,2,0))\n",
    "        return latent, reconstructed\n",
    "\n",
    "    def get_loss(self, batch):\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        latent, reconstructed = self.forward(x)\n",
    "        loss = nn.functional.mse_loss(x, reconstructed)\n",
    "        loss = torch.mean(loss)\n",
    "        if debug:\n",
    "            print('loss is:')\n",
    "            print(loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch)\n",
    "        #implement log loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def represent(self):\n",
    "        print('encoder is:')\n",
    "        print(self.encoder)\n",
    "        print('decoder is:')\n",
    "        print(self.decoder)\n",
    "\n",
    "net = Autoencoder()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder is:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      ")\n",
      "decoder is:\n",
      "Sequential(\n",
      "  (0): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 23.7 K\n",
      "1 | decoder | Sequential | 23.7 K\n",
      "---------------------------------------\n",
      "47.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.4 K    Total params\n",
      "0.189     Total estimated model params size (MB)\n",
      "/Users/gpnuser/Documents/Horos Data/Jas_Pycharm/PycharmProject/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net.represent()\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs)\n",
    "    trainer.fit(net, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
